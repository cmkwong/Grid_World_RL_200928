step_cost=-0.001 state_mode='Q', agent_mode="MC", sampling_times=1000(update by mean)
= = = = = = = = = = = = = = = = = = = = = = = =
-------------Action Value Table--------------
[0.0, -0.0, 0.0, -0.01] [-0.01, -0.0, -0.0, -0.0] [-0.01, -0.01, 0.0, -0.01] [-0.0, -0.0, -0.59, -0.01] 
[0.0, 0.0, 0.0, 0.0] [0, 0, 0, 0] [-0.01, -0.99, 0.01, -0.04] [-0.03, -0.1, 0.0, -0.14] 
[-0.0, 0.01, 0.01, 0.0] [0.01, 0.01, 0.01, 0.01] [-0.01, 0.0, 0.01, 0.01] [0, 0, 0, 0] 
[0.0, 0.01, 0.0, 0.01] [0.01, 0.01, 0.01, 0.01] [0.0, 0.0, 0.01, 0.01] [0, 0, 0, 0] 
--------------Policy Table-------------
[0.22, 0.08, 0.64, 0.06] [0.32, 0.15, 0.23, 0.3] [0.07, 0.19, 0.42, 0.33] [0.11, 0.43, 0.0, 0.47] 
[0.1, 0.12, 0.66, 0.12] [0.25, 0.25, 0.25, 0.25] [0.51, 0.0, 0.47, 0.01] [0.0, 0.0, 1.0, 0.0] 
[0.04, 0.45, 0.44, 0.08] [0.13, 0.22, 0.44, 0.2] [0.0, 0.0, 0.95, 0.04] [0.25, 0.25, 0.25, 0.25] 
[0.06, 0.58, 0.12, 0.24] [0.06, 0.76, 0.08, 0.1] [0.01, 0.0, 0.98, 0.0] [0.25, 0.25, 0.25, 0.25] 
300 plays - mean step: 253.60; completed goal: 95.67%; reward per play: 0.66

step_cost=-0.001 state_mode='Q', agent_mode="MC", sampling_times=1000
= = = = = = = = = = = = = = = = = = = = = = = =
-------------Action Value Table--------------
[0.15, 0.13, 0.19, 0.15] [0.16, 0.15, 0.2, 0.16] [0.13, -0.04, 0.16, 0.14] [-0.17, -0.13, -1.0, -0.12] 
[0.12, 0.2, 0.24, 0.2] [0, 0, 0, 0] [0.08, -1.0, 0.2, 0.26] [-0.01, -0.01, 0.0, -0.01] 
[0.21, 0.27, 0.29, 0.21] [0.21, 0.19, 0.37, 0.26] [0.15, 0.0, 0.42, 0.23] [0, 0, 0, 0] 
[0.23, 0.36, 0.31, 0.29] [0.29, 0.42, 0.39, 0.33] [0.1, 0.0, 0.44, 0.32] [0, 0, 0, 0] 
--------------Policy Table-------------
[0.12, 0.12, 0.52, 0.24] [0.13, 0.29, 0.15, 0.43] [0.18, 0.07, 0.46, 0.29] [0.26, 0.4, 0.0, 0.34] 
[0.09, 0.1, 0.63, 0.19] [0.25, 0.25, 0.25, 0.25] [0.13, 0.0, 0.62, 0.25] [0.0, 0.0, 1.0, 0.0] 
[0.07, 0.3, 0.46, 0.16] [0.08, 0.27, 0.57, 0.08] [0.07, 0.13, 0.74, 0.05] [0.25, 0.25, 0.25, 0.25] 
[0.06, 0.59, 0.21, 0.15] [0.09, 0.83, 0.04, 0.04] [0.01, 0.07, 0.92, 0.01] [0.25, 0.25, 0.25, 0.25] 
300 plays - mean step: 586.70; completed goal: 95.00%; reward per play: 0.64

step_cost=-0.001 state_mode='Q', agent_mode="DP", sampling_times=1000
= = = = = = = = = = = = = = = = = = = = = = = =
-------------Action Value Table--------------
[0.99, 0.99, 0.99, 0.99] [0.99, 1.0, 0.99, 0.99] [1.0, 0.99, 1.0, 0.99] [0.99, 0.99, 1.0, 1.0] 
[0.99, 0.99, 1.0, 0.99] [0.0, 0.0, 0.0, 0.0] [1.0, 1.0, 1.0, 1.0] [0.99, 1.0, -1.0, 1.0] 
[0.99, 1.0, 1.0, 1.0] [1.0, 1.0, 1.0, 1.0] [1.0, -1.0, 1.0, 1.0] [0, 0, 0, 0] 
[1.0, 1.0, 1.0, 1.0] [1.0, 1.0, 1.0, 1.0] [1.0, 1.0, 1.0, 1.0] [0, 0, 0, 0] 
--------------Policy Table-------------
[0.0, 0.0, 1.0, 0.0] [0.0, 1.0, 0.0, 0.0] [0.0, 0.0, 1.0, 0.0] [0.0, 0.0, 0.0, 1.0] 
[0.0, 0.0, 1.0, 0.0] [0.25, 0.25, 0.25, 0.25] [0.0, 0.0, 1.0, 0.0] [0.0, 0.0, 0.0, 1.0] 
[0.0, 1.0, 0.0, 0.0] [0.0, 0.0, 1.0, 0.0] [0.0, 0.0, 1.0, 0.0] [0.25, 0.25, 0.25, 0.25] 
[0.0, 1.0, 0.0, 0.0] [0.0, 1.0, 0.0, 0.0] [0.0, 1.0, 0.0, 0.0] [0.25, 0.25, 0.25, 0.25] 
200 plays - mean step: 6.00; completed goal: 99.00%; reward per play: 0.97

step_cost=-0.001 state_mode='V', agent_mode="DP", sampling_times=1000
= = = = = = = = = = = = = = = = = = = = = = = =
------------Value Table---------------
0.59   0.65   0.73   0.65   
0.65   0.0   0.81   0.73   
0.73   0.81   0.9   0.0   
0.81   0.9   1.0   0.0   
--------------Policy Table-------------
[0.0, 0.0, 1.0, 0.0] [0.0, 1.0, 0.0, 0.0] [0.0, 0.0, 1.0, 0.0] [0.0, 0.0, 0.0, 1.0] 
[0.0, 0.0, 1.0, 0.0] [0.25, 0.25, 0.25, 0.25] [0.0, 0.0, 1.0, 0.0] [0.0, 0.0, 0.0, 1.0] 
[0.0, 1.0, 0.0, 0.0] [0.0, 0.0, 1.0, 0.0] [0.0, 0.0, 1.0, 0.0] [0.25, 0.25, 0.25, 0.25] 
[0.0, 1.0, 0.0, 0.0] [0.0, 1.0, 0.0, 0.0] [0.0, 1.0, 0.0, 0.0] [0.25, 0.25, 0.25, 0.25] 
210 plays - mean step: 6.00; completed goal: 98.10%; reward per play: 0.96

step_cost=-0.001 state_mode='V', agent_mode="MC", sampling_times=1000
= = = = = = = = = = = = = = = = = = = = = = = =
------------Value Table---------------
0.59   0.65   0.73   0.65   
0.65   0.0   0.81   0.73   
0.73   0.81   0.9   0.0   
0.81   0.9   1.0   0.0   
--------------Policy Table-------------
[0.0, 0.06, 0.94, 0.0] [0.0, 1.0, 0.0, 0.0] [0.0, 0.0, 1.0, 0.0] [0.0, 0.0, 0.63, 0.37] 
[0.0, 0.0, 1.0, 0.0] [0.25, 0.25, 0.25, 0.25] [0.0, 0.0, 1.0, 0.0] [0.0, 0.0, 0.0, 1.0] 
[0.0, 0.85, 0.15, 0.0] [0.0, 0.61, 0.39, 0.0] [0.0, 0.0, 1.0, 0.0] [0.25, 0.25, 0.25, 0.25] 
[0.0, 1.0, 0.0, 0.0] [0.0, 1.0, 0.0, 0.0] [0.0, 1.0, 0.0, 0.0] [0.25, 0.25, 0.25, 0.25] 
200 plays - mean step: 6.00; completed goal: 99.00%; reward per play: 0.97

